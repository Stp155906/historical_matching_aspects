# -*- coding: utf-8 -*-
"""Historical_Matching_Aspects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L15XhmJDJw1amgJTd3PEsPkq8QaFPcGm
"""

import requests
import json

# Fetching the raw text data from the URL
url = "https://raw.githubusercontent.com/Stp155906/weeks-historical-celestial-forecast/main/weeks_historical_celestial_forecast.json"
response = requests.get(url)

# Processing the data line by line
if response.status_code == 200:
    raw_data = response.text.strip().splitlines()  # Split by lines
    data = [json.loads(line) for line in raw_data]  # Parse each line as JSON
    print("Data processed successfully!")
else:
    print("Failed to fetch data.")

# Extracting the 'aspect_key' and 'date' from each entry
aspect_date_list = [{"aspect_key": entry["aspect_key"], "date": entry["date"]} for entry in data]

# Displaying the extracted data
for item in aspect_date_list:
    print(item)

"""Step 4: Fetch Data from the New API

First, let’s fetch the data from the new API, similar to how we did with the first JSON file. This data is structured by year, and we’ll need to extract the relevant events.

Here’s the code to fetch and process the data from the new API:
"""

# Fetching the JSON data from the new URL
url_new = "https://raw.githubusercontent.com/Stp155906/refined-historical-events/main/cleaned_historical_events_100_years.json"
response_new = requests.get(url_new)

# Checking if the request was successful and processing the data
if response_new.status_code == 200:
    data_new = response_new.json()
    print("New data fetched and processed successfully!")
else:
    print("Failed to fetch new data.")



"""Step 5: Combine Data Based on Year and Category

Now that we have both datasets, the next step is to combine the data from the two sources. Specifically, we will match the date from the first dataset with the corresponding year in the new dataset and then combine the data by categories (such as commerce, war, etc.).

"""

# Function to extract year from the date
def extract_year(date_str):
    return date_str.split("-")[0]

# Combine the data based on year and category
combined_data = []

for entry in aspect_date_list:
    year = extract_year(entry["date"])
    if year in data_new:
        for category, events in data_new[year].items():
            combined_data.append({
                "aspect_key": entry["aspect_key"],
                "date": entry["date"],
                "year": year,
                "category": category,
                "events": events
            })

# Display the combined data
for item in combined_data:
    print(item)

"""Step 6: Fetch and Display This Week’s Celestial Aspects

Let’s fetch this week’s celestial aspects from the provided API, and then we’ll match them to the previously fetched data to showcase any similar aspects that have occurred in the past along with corresponding events.

Here’s how to fetch the weekly aspects:
"""

# Fetching the weekly aspects data
url_weekly_aspects = "https://raw.githubusercontent.com/Stp155906/weekly-aspects/main/weekly_aspects.json"
response_weekly_aspects = requests.get(url_weekly_aspects)

# Checking if the request was successful and processing the data
if response_weekly_aspects.status_code == 200:
    weekly_aspects_data = response_weekly_aspects.json()["weekly_aspects"]
    print("Weekly aspects data fetched successfully!")
else:
    print("Failed to fetch weekly aspects data.")

"""Step 7: Match Weekly Aspects with Historical Data

Now that we have the weekly aspects, we can match them with the previously fetched historical data and display any corresponding events.

Here’s the code to do that:
"""

# Function to match aspects
def match_aspects(weekly_aspects, historical_data, week_date):
    matches = []
    for aspect in weekly_aspects:
        for past_event in historical_data:
            if (aspect["planet1"] in past_event["aspect_key"] and aspect["planet2"] in past_event["aspect_key"]) or \
               (aspect["planet2"] in past_event["aspect_key"] and aspect["planet1"] in past_event["aspect_key"]):
                matches.append({
                    "weekly_date": week_date,
                    "aspect": f'{aspect["planet1"]}-{aspect["planet2"]}-{aspect["aspect"]}',
                    "past_date": past_event["date"],
                    "category": past_event["category"],
                    "events": past_event["events"]
                })
    return matches

# Get this week's forecast and match it with historical data
weekly_forecast_matches = []
for week in weekly_aspects_data:
    matched_aspects = match_aspects(week["aspects"], combined_data, week["date"])
    weekly_forecast_matches.extend(matched_aspects)

# Display the matched aspects and events
for match in weekly_forecast_matches:
    print(f"On {match['weekly_date']}, the aspect {match['aspect']} will occur.")
    print(f"This aspect has occurred previously on {match['past_date']} with the following events under the category '{match['category']}':")
    for event in match["events"]:
        print(f"- {event}")
    print("\n")

"""Step 8 (Updated): Save the Combined Data as a JSON File with a New Name"""

import json
import os
import math

# Define the new file name
output_file_name = 'historical_matching_aspects.json'

# Save the data to a JSON file
with open(output_file_name, 'w') as json_file:
    json.dump(weekly_forecast_matches, json_file, indent=4)

print(f"Data saved to {output_file_name} successfully!")

# Get the file size in bytes
file_size = os.path.getsize(output_file_name)

# Convert the size to a more readable format (e.g., MB)
file_size_mb = file_size / (1024 * 1024)

print(f"File size: {file_size_mb:.2f} MB")

# Check if the file size exceeds 50MB
if file_size_mb > 50:
    # Number of parts to split the file into (50MB per part)
    num_parts = math.ceil(file_size_mb / 50)  # Roughly split into parts under 50MB

    # Split the data
    chunk_size = len(weekly_forecast_matches) // num_parts

    index_file = {
        "parts": []
    }

    for i in range(num_parts):
        chunk = weekly_forecast_matches[i*chunk_size : (i+1)*chunk_size]
        chunk_filename = f'historical_matching_aspects_part_{i+1}.json'
        with open(chunk_filename, 'w') as chunk_file:
            json.dump(chunk, chunk_file, indent=4)
        print(f"Created {chunk_filename}")
        index_file["parts"].append(chunk_filename)

    # Save the index file
    index_filename = 'historical_matching_aspects_index.json'
    with open(index_filename, 'w') as index_file_json:
        json.dump(index_file, index_file_json, indent=4)

    print(f"Created index file {index_filename}")
else:
    print("File size is under 50MB, no need to split.")